import logging
import os
import tempfile

import gradio as gr  # type: ignore
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer  # type: ignore

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Loading model and tokenizer...")

# --- 1. Load Model and Tokenizer (Done only once at startup) ---
model_name = "./model"
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
logger.info(f"Using device: {device}")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name,
    attn_implementation="eager",  # Mac/MPS å¿…é¡»ç”¨ eager
    trust_remote_code=True,
    use_safetensors=True,
)
# å¯åŠ¨æ—¶å°±æŠŠæ¨¡å‹ç§»åˆ° MPS/CPU å¹¶è½¬æ¢ä¸º float32ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚é‡å¤åŠ è½½
model = model.eval().to(device).to(torch.float32)
logger.info("âœ… Model loaded successfully and moved to %s.", device)


# --- 2. Main Processing Function (å®Œå–„åç‰ˆæœ¬ï¼šç»“åˆç®€æ´è¾“å‡º + å¤šä»»åŠ¡ + è¾¹ç•Œæ¡†ç»˜åˆ¶) ---
def process_ocr_task(
    image: Image.Image,
    model_size: str,
    task_type: str,
    ref_text: str,
    is_eval_mode: bool,
) -> tuple[Image.Image | None, str, str]:
    """
    å¤„ç†å›¾åƒå¹¶è¿”å›å¤šç§è¾“å‡ºæ ¼å¼ï¼š
    - å¸¦è¾¹ç•Œæ¡†çš„æ ‡æ³¨å›¾åƒï¼ˆè‹¥æ£€æµ‹åˆ°åæ ‡ï¼‰
    - Markdown å†…å®¹ï¼ˆè‹¥ä»»åŠ¡ç”Ÿæˆï¼‰
    - çº¯æ–‡æœ¬ OCR ç»“æœ
    """
    if image is None:
        return None, "Please upload an image first.", "Please upload an image first."

    try:
        logger.info("ğŸš€ Running inference on %s...", device)
        # æ¨¡å‹å·²åœ¨å¯åŠ¨æ—¶ç§»åˆ° deviceï¼Œç›´æ¥ä½¿ç”¨

        with tempfile.TemporaryDirectory() as output_path:
            # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½® prompt
            if task_type == "ğŸ“ Free OCR":
                prompt = "<image>\nFree OCR."
            elif task_type == "ğŸ“„ Convert to Markdown":
                prompt = "<image>\n<|grounding|>Convert the document to markdown."
            elif task_type == "ğŸ“ˆ Parse Figure":
                prompt = "<image>\nParse the figure."
            elif task_type == "ğŸ” Locate Object by Reference":
                if not ref_text or ref_text.strip() == "":
                    return (
                        None,
                        "âŒ Reference text is required for Locate task.",
                        "âŒ Reference text is required for Locate task.",
                    )
                prompt = (
                    f"<image>\nLocate <|ref|>{ref_text.strip()}<|/ref|> in the image."
                )
            else:
                prompt = "<image>\nFree OCR."

            # ä¿å­˜ä¸Šä¼ çš„å›¾åƒ
            temp_image_path = os.path.join(output_path, "temp_image.png")
            image.save(temp_image_path)

            # é…ç½®æ¨¡å‹å°ºå¯¸å‚æ•°
            size_configs = {
                "Tiny": {"base_size": 512, "image_size": 512, "crop_mode": False},
                "Small": {"base_size": 640, "image_size": 640, "crop_mode": False},
                "Base": {"base_size": 1024, "image_size": 1024, "crop_mode": False},
                "Large": {"base_size": 1280, "image_size": 1280, "crop_mode": False},
                "Gundam (Recommended)": {
                    "base_size": 1024,
                    "image_size": 640,
                    "crop_mode": True,
                },
            }
            config = size_configs.get(model_size, size_configs["Gundam (Recommended)"])

            logger.info("ğŸƒ Running inference with prompt: %s", prompt)
            plain_text_result = model.infer(
                tokenizer,
                prompt=prompt,
                image_file=temp_image_path,
                output_path=output_path,
                base_size=config["base_size"],
                image_size=config["image_size"],
                crop_mode=config["crop_mode"],
                save_results=True,
                test_compress=True,
                eval_mode=is_eval_mode,
            )

            logger.info(
                "ğŸ“„ Text result (length=%d)",
                len(plain_text_result) if plain_text_result else 0,
            )

            # è¯»å–ç”Ÿæˆçš„ markdown æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            markdown_result_path = os.path.join(output_path, "result.mmd")
            markdown_content = ""
            if os.path.exists(markdown_result_path):
                with open(markdown_result_path, encoding="utf-8") as f:
                    markdown_content = f.read()
            else:
                markdown_content = "Markdown result was not generated. This is expected for 'Free OCR' task."

            # è¯»å–æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆçš„æ ‡æ³¨å›¾åƒï¼ˆè‹¥å­˜åœ¨ï¼‰
            result_image_pil = None
            image_result_path = os.path.join(output_path, "result_with_boxes.jpg")
            if os.path.exists(image_result_path):
                result_image_pil = Image.open(image_result_path)
                result_image_pil.load()
                logger.info("âœ… Found annotated image: %s", image_result_path)
            else:
                logger.info("â„¹ï¸ No annotated image generated (expected for some tasks).")

            # è¿”å›ä¸‰ä¸ªè¾“å‡ºï¼šæ ‡æ³¨å›¾åƒã€Markdown é¢„è§ˆå†…å®¹ã€çº¯æ–‡æœ¬ç»“æœ
            text_result = plain_text_result if plain_text_result else markdown_content
            return result_image_pil, markdown_content, text_result

    except Exception as e:
        logger.exception("âŒ Inference failed")
        error_msg = f"ERROR: {e}"
        return None, error_msg, error_msg


# --- 3. Build the Gradio Interface (å®Œå–„åç‰ˆæœ¬ï¼šTab å±•ç¤º + eval_mode æ§åˆ¶) ---
with gr.Blocks(title="ğŸ³ DeepSeek-OCR", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ³ DeepSeek-OCR Demo (Mac/MPS é€‚é…ç‰ˆ)

        **ğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š**
        1. **ä¸Šä¼ å›¾ç‰‡** åˆ°ä¸Šä¼ æ¡†ã€‚
        2. é€‰æ‹© **åˆ†è¾¨ç‡**ã€‚æ¨èä½¿ç”¨ `Gundam` ä»¥è·å¾—æ–‡æ¡£æœ€ä½³æ•ˆæœã€‚
        3. é€‰æ‹© **ä»»åŠ¡ç±»å‹**ï¼š
            - **ğŸ“ Free OCR**ï¼šæå–åŸå§‹æ–‡æœ¬
            - **ğŸ“„ Convert to Markdown**ï¼šè½¬æ¢ä¸º Markdown æ ¼å¼ï¼ˆä¿ç•™ç»“æ„ï¼‰
            - **ğŸ“ˆ Parse Figure**ï¼šæå–å›¾è¡¨ç»“æ„åŒ–æ•°æ®
            - **ğŸ” Locate Object by Reference**ï¼šå®šä½ç‰¹å®šå¯¹è±¡/æ–‡æœ¬ï¼ˆéœ€å¡«å†™å‚è€ƒæ–‡æœ¬ï¼‰
        4. å¯é€‰ï¼šå‹¾é€‰ **Evaluation Mode** ä»¥ä»…è¿”å›çº¯æ–‡æœ¬ï¼ˆå¯èƒ½æ›´å¿«ï¼Œä½†ä¸ç”Ÿæˆæ ‡æ³¨å›¾åƒå’Œ Markdownï¼‰ã€‚
        """
    )

    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(type="pil", label="ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡")

            model_size = gr.Dropdown(
                choices=["Tiny", "Small", "Base", "Large", "Gundam (Recommended)"],
                value="Gundam (Recommended)",
                label="âš™ï¸ åˆ†è¾¨ç‡å¤§å°",
            )

            task_type = gr.Dropdown(
                choices=[
                    "ğŸ“ Free OCR",
                    "ğŸ“„ Convert to Markdown",
                    "ğŸ“ˆ Parse Figure",
                    "ğŸ” Locate Object by Reference",
                ],
                value="ğŸ“„ Convert to Markdown",
                label="ğŸš€ ä»»åŠ¡ç±»å‹",
            )

            ref_text_input = gr.Textbox(
                label="ğŸ“ å‚è€ƒæ–‡æœ¬ï¼ˆç”¨äº Locate ä»»åŠ¡ï¼‰",
                placeholder="ä¾‹å¦‚: the teacher, 20-10, a red car...",
                visible=False,
            )

            eval_mode_checkbox = gr.Checkbox(
                value=False,
                label="å¯ç”¨ Evaluation Mode",
                info="ä»…è¿”å›çº¯æ–‡æœ¬ï¼Œå¯èƒ½æ›´å¿«ã€‚å–æ¶ˆå‹¾é€‰å¯è·å¾—æ ‡æ³¨å›¾åƒå’Œ Markdownã€‚",
            )

            submit_btn = gr.Button("ğŸš€ å¤„ç†å›¾ç‰‡", variant="primary")

        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.TabItem("ğŸ“· æ ‡æ³¨å›¾åƒ"):
                    output_image = gr.Image(
                        interactive=False, label="å¸¦è¾¹ç•Œæ¡†çš„å›¾åƒï¼ˆå¦‚æœ‰æ£€æµ‹åˆ°ï¼‰"
                    )
                with gr.TabItem("ğŸ“ Markdown é¢„è§ˆ"):
                    output_markdown = gr.Markdown(label="Markdown æ¸²æŸ“é¢„è§ˆ")
                with gr.TabItem("ğŸ“„ Markdown æºç  / çº¯æ–‡æœ¬è¾“å‡º"):
                    output_text = gr.Textbox(
                        lines=20,
                        show_copy_button=True,
                        interactive=False,
                        label="çº¯æ–‡æœ¬ç»“æœæˆ– Markdown æºç ",
                    )

    # --- UI äº¤äº’é€»è¾‘ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤º/éšè—å‚è€ƒæ–‡æœ¬æ¡† ---
    def toggle_ref_text_visibility(task: str) -> gr.Textbox:
        return (
            gr.Textbox(visible=True)
            if task == "ğŸ” Locate Object by Reference"
            else gr.Textbox(visible=False)
        )

    task_type.change(
        fn=toggle_ref_text_visibility, inputs=task_type, outputs=ref_text_input
    )

    submit_btn.click(
        fn=process_ocr_task,
        inputs=[image_input, model_size, task_type, ref_text_input, eval_mode_checkbox],
        outputs=[output_image, output_markdown, output_text],
    )

# --- 4. Launch the App ---
if __name__ == "__main__":
    if not os.path.exists("examples"):
        os.makedirs("examples")
    demo.launch()
